{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用的package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from scipy.spatial import distance\n",
    "from scipy.linalg import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map/Reduce function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這份程式碼中與Kmeans_Euclidean不一樣的地方只有在initial_clustering_map()與k_means_map()中以曼哈頓距離計算loss function與衡量資料間的距離。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readfile(line):\n",
    "    wordlist=line.split(\"\\n\")\n",
    "    maplist=[]\n",
    "    for item in wordlist:\n",
    "        s = item.split(\" \")\n",
    "        data=[]\n",
    "        for item in s:\n",
    "            a=float(item)\n",
    "            data.append(a)\n",
    "        maplist.append((norm(data),data)) # add to map\n",
    "    return maplist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_clustering_map(x):\n",
    "    d= []\n",
    "    for center in x[2]:\n",
    "        dis = distance.cityblock(x[1], center)\n",
    "        d.append(dis)\n",
    "    cluster = d.index(min(d))\n",
    "    return(x[0], (cluster, x[1], x[2][cluster], min(d)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means_map(x):\n",
    "    d = []\n",
    "    for center in x[2]:\n",
    "        dis = distance.cityblock(x[1][1], center)**2\n",
    "        d.append(dis)\n",
    "    \n",
    "    min_d = min(d)\n",
    "    cluster = d.index(min_d)\n",
    "    if (min_d < x[1][3]):\n",
    "        return (x[0], (cluster, x[1][1], x[2][cluster], min_d))\n",
    "    else:\n",
    "        return (x[0], x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_centroid_mapper(x):\n",
    "    z=[]\n",
    "    for component in x[1]:\n",
    "        z.append(component/num_cluster_dict[x[0]])\n",
    "    return (x[0], z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setMaster(\"local\").setAppName(\"wordcount\") # call sparkconf\n",
    "conf = SparkConf().set(\"spark.default.parallelism\", 4)\n",
    "sc = SparkContext(conf=conf) # call sparkcontext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_intial = sc.textFile(\"data.txt\").flatMap(readfile) # read txt file [||p_k||, p_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = sc.textFile(\"c1.txt\").flatMap(readfile) # read txt file\n",
    "c1 = c1.map(lambda x: x[1])\n",
    "c1 = c1.collect() # list of centroids [c_1, c_2, ..., c_10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = [] # value of loss function in each iteration\n",
    "cluster_dict = [] # number of points in each cluster in each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_intial.map(lambda x: [x[0], x[1], c1]) # [||p_k||, p_k, [c_1, c_2, ..., c_10]]\n",
    "dataset = dataset.map(initial_clustering_map) # [||p_k||, (m, p_k, c_m, ||p_k-c_m||)], m為cluster編號            \n",
    "R = dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (1,21): # 20次要設21\n",
    "    \n",
    "    # 將新的c接回去\n",
    "    R = R.map(lambda x: [x[0], x[1], c1]) # [||p_k||, (m, p_k, c_m, ||p_k-c_m||), [c_1, c_2, ..., c_10]]\n",
    "    \n",
    "    # 重新分群\n",
    "    R = R.map(k_means_map) # [||p_k||, (m, p_k, c_m, ||p_k-c_m||)]\n",
    "    D = R.map(lambda x: (x[1][0], x[1][3])) # (m, ||p_k-c_m||)\n",
    "    \n",
    "    loss = D.values().sum()\n",
    "    loss_function.append(loss)\n",
    "    \n",
    "    new_centroid = R.map(lambda x: (x[1][0], x[1][1])) # (m, p_k)\n",
    "    num_cluster_dict = new_centroid.countByKey()\n",
    "    cluster_dict.append(num_cluster_dict)\n",
    "    \n",
    "    new_centroid = new_centroid.reduceByKey(lambda x, y: [t1+t2 for t1, t2 in zip(x, y)]) # element-wise adding\n",
    "    new_centroid = new_centroid.map(new_centroid_mapper) # (m, c_m)\n",
    "    c1 = new_centroid.values().collect() # list of centroids [c_1, c_2, ..., c_10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case of c2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_centroid_mapper2(x):\n",
    "    z=[]\n",
    "    for component in x[1]:\n",
    "        z.append(component/num_cluster_dict2[x[0]])\n",
    "    return (x[0], z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "c2 = sc.textFile(\"c2.txt\").flatMap(readfile) # read txt file\n",
    "c2 = c2.map(lambda x: x[1])\n",
    "c2 = c2.collect() # list of centroids [c_1, c_2, ..., c_10]\n",
    "loss_function2 = [] # value of loss function in each iteration\n",
    "cluster_dict2 = [] # number of points in each cluster in each iteration\n",
    "dataset = dataset_intial.map(lambda x: [x[0], x[1], c2]) # [||p_k||, p_k, [c_1, c_2, ..., c_10]]\n",
    "dataset = dataset.map(initial_clustering_map) # [||p_k||, (m, p_k, c_m, ||p_k-c_m||)], m為cluster編號            \n",
    "R = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (1,21): # 20次要設21\n",
    "    \n",
    "    # 將新的c接回去\n",
    "    R = R.map(lambda x: [x[0], x[1], c2]) # [||p_k||, (m, p_k, c_m, ||p_k-c_m||), [c_1, c_2, ..., c_10]]\n",
    "    \n",
    "    # 重新分群\n",
    "    R = R.map(k_means_map) # [||p_k||, (m, p_k, c_m, ||p_k-c_m||)]\n",
    "    D = R.map(lambda x: (x[1][0], x[1][3])) # (m, ||p_k-c_m||)\n",
    "    \n",
    "    loss = D.values().sum()\n",
    "    loss_function2.append(loss)\n",
    "    \n",
    "    new_centroid = R.map(lambda x: (x[1][0], x[1][1])) # (m, p_k)\n",
    "    num_cluster_dict2 = new_centroid.countByKey()\n",
    "    cluster_dict2.append(num_cluster_dict2)\n",
    "    \n",
    "    new_centroid = new_centroid.reduceByKey(lambda x, y: [t1+t2 for t1, t2 in zip(x, y)]) # element-wise adding\n",
    "    new_centroid = new_centroid.map(new_centroid_mapper2) # (m, c_m)\n",
    "    c2 = new_centroid.values().collect() # list of centroids [c_1, c_2, ..., c_10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[550117.1419999995, 464829.2684039448, 470934.15384668094, 483874.81628509343, 489234.2347883463, 487664.6926267904, 483718.6659285149, 475337.9476330566, 474871.96654965664, 457244.7897417528, 447493.1956040521, 450891.8358047706, 451232.57747569657, 451860.12588546576, 451567.2235891488, 452710.05209994374, 453078.22696184996, 450646.1355620941, 450419.9701134367, 449009.59037188545]\n"
     ]
    }
   ],
   "source": [
    "print(loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1433739.2192009955, 1433739.2192009955, 1084488.7769648773, 973431.7146620404, 895934.5925630709, 865128.3352940814, 845846.647031348, 827219.5827561249, 803590.3456011118, 756039.5172761207, 717332.9025432297, 694587.9252526882, 684444.5019967903, 674574.7475478561, 667409.4699160281, 663556.6278215044, 660162.7772287563, 656041.3222947121, 653036.7540731612, 651112.4262522729, 649689.0131843555]\n"
     ]
    }
   ],
   "source": [
    "print(loss_function2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# 開啟輸出的 CSV 檔案\n",
    "with open('loss_function_cityblock.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(loss_function)\n",
    "    writer.writerow(loss_function2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "eucl_of_c1 = [] # 利用曼哈頓距離計算loss function時，歐式距離下，c1中centroid彼此的距離\n",
    "for i in range(0,10):\n",
    "    eucl_of_c1.append([])\n",
    "    for j in range(0,10):\n",
    "        eucl_of_c1[i].append(distance.euclidean(c1[i],c1[j]))\n",
    "\n",
    "eucl_of_c2 = [] # 利用曼哈頓距離計算loss function時，歐式距離下，c2中centroid彼此的距離\n",
    "for i in range(0,10):\n",
    "    eucl_of_c2.append([])\n",
    "    for j in range(0,10):\n",
    "        eucl_of_c2[i].append(distance.euclidean(c2[i],c2[j]))\n",
    "        \n",
    "manh_of_c1 = [] # 利用曼哈頓距離計算loss function時，曼哈頓距離下，c1中centroid彼此的距離\n",
    "for i in range(0,10):\n",
    "    manh_of_c1.append([])\n",
    "    for j in range(0,10):\n",
    "        manh_of_c1[i].append(distance.cityblock(c1[i],c1[j]))\n",
    "\n",
    "manh_of_c2 = [] # 利用曼哈頓距離計算loss function時，曼哈頓距離下，c2中centroid彼此的距離\n",
    "for i in range(0,10):\n",
    "    manh_of_c2.append([])\n",
    "    for j in range(0,10):\n",
    "        manh_of_c2[i].append(distance.cityblock(c2[i],c2[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('manh_c1_euclidean.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerows(eucl_of_c1)\n",
    "\n",
    "with open('manh_c2_euclidean.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerows(eucl_of_c2)\n",
    "    \n",
    "with open('manh_c1_manh.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerows(manh_of_c1)\n",
    "\n",
    "with open('manh_c2_manh.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerows(manh_of_c2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
